{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DayenaJeong/FS_neuron/blob/main/FSneuron_poisson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FzrHRqWjMc75"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nqDodwMlMiEw"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "\n",
        "def softplus(x, beta=1):\n",
        "    return (1 / beta) * np.log(1 + np.exp(beta * x))\n",
        "\n",
        "\n",
        "def mish(x, beta=1):\n",
        "    return x * np.tanh(softplus(x, beta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JXTXUEYJL7zb"
      },
      "outputs": [],
      "source": [
        "# Implementation of spike function for PyTorch custom gradient\n",
        "class SpikeFunction(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v_scaled):\n",
        "        z_ = torch.where(v_scaled > 0, torch.ones_like(v_scaled), torch.zeros_like(v_scaled))\n",
        "        ctx.save_for_backward(v_scaled)\n",
        "        return z_\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        v_scaled, = ctx.saved_tensors\n",
        "        dz_dv_scaled = torch.maximum(1 - torch.abs(v_scaled), torch.tensor(0.0, device=v_scaled.device))\n",
        "        dE_dv_scaled = grad_output * dz_dv_scaled\n",
        "        return dE_dv_scaled\n",
        "\n",
        "# Call spike function for PyTorch\n",
        "def spike_function(v_scaled):\n",
        "    return SpikeFunction.apply(v_scaled)\n",
        "\n",
        "# FS class definition\n",
        "class FS(nn.Module):\n",
        "    def __init__(self, num_params):\n",
        "        super(FS, self).__init__()\n",
        "\n",
        "        if num_params == 4:\n",
        "            h_base = torch.tensor([5.1285, 2.5128, 1.2828, 0.5581])\n",
        "            d_base = torch.tensor([5.1790, 2.5725, 1.2913, 0.6380])\n",
        "            T_base = torch.tensor([4.8367, 2.2886, 1.0585, 0.4028])\n",
        "        elif num_params == 8:\n",
        "            h_base = torch.tensor([1.4519, 7.0948, 3.2787, 1.5279, 1.0512, 0.5631, 0.2835, 0.5458])\n",
        "            d_base = torch.tensor([-0.1170, 7.1730, 3.3550, 1.5654, 1.0538, 0.5489, 0.2754, 0.1355])\n",
        "            T_base = torch.tensor([-2.0254, 5.4881, 1.6739, 0.3054, -0.4375, -0.9967, -1.2781, -1.4162])\n",
        "        elif num_params == 12:\n",
        "            h_base = torch.tensor([1.1334, 3.6824, 2.5663, 2.7249, 1.1612, 0.8382, 0.6529, 0.3695, 0.1317, 0.0875, 0.0582, 0.3141])\n",
        "            d_base = torch.tensor([-0.1226, 3.7413, 2.5976, 2.7225, 1.1805, 0.8291, 0.6337, 0.3521, 0.1284, 0.0903, 0.0629, 0.0391])\n",
        "            T_base = torch.tensor([-1.9316, 2.3994, 1.3145, 2.1865, 0.4599, -0.0309, -0.4670, -0.8735, -1.0898, -1.1210, -1.1446, -1.1642])\n",
        "        elif num_params == 16:\n",
        "            h_base = torch.tensor([0.0424, 1.1436, 0.2666, 1.1882, 2.7916, 0.2399, 1.7131, 1.4285, 0.3389, 0.6917, 0.3394, 0.1824, 0.1194, -0.2559, 1.9220, 0.7656])\n",
        "            d_base = torch.tensor([-0.0902, 1.2459, 0.1350, 1.2281, 2.7910, 0.1598, 1.7130, 1.4286, -0.0517, 0.6976, 0.3375, 0.1827, 0.1158, 0.0121, 0.0668, 0.0262])\n",
        "            T_base = torch.tensor([-1.5501, 1.7340, -0.0669, 1.1439, 2.7409, -0.0463, 1.4159, 1.1298, -2.3731, 0.1034, -0.2793, -0.4198, -0.4929, -0.5724, -0.2767, -0.4566])\n",
        "\n",
        "        # Use Laplace noise\n",
        "        self.h = nn.Parameter(h_base + torch.distributions.Laplace(0, 0.1).sample(h_base.size()))\n",
        "        self.d = nn.Parameter(d_base + torch.distributions.Laplace(0, 0.1).sample(d_base.size()))\n",
        "        self.T = nn.Parameter(T_base + torch.distributions.Laplace(0, 0.1).sample(T_base.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        v = x.clone()\n",
        "\n",
        "        # Initialize temporary output for FS spike neural network\n",
        "        temp_out = torch.zeros_like(v)\n",
        "\n",
        "        # Implement FS spike neural network\n",
        "        for t in range(len(self.T)):\n",
        "            #v_scaled = (v - self.T[t]) / (torch.abs(v) + 1)\n",
        "            v_scaled = v - self.T[t]\n",
        "            z = spike_function(v_scaled)\n",
        "            temp_out += z * self.d[t]\n",
        "            v = v - z * self.h[t]\n",
        "\n",
        "        return temp_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFk_lDrBMj5W"
      },
      "source": [
        "# GELU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe0LX_n1MrUN"
      },
      "source": [
        "K=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mQ7Om56kMnLH",
        "outputId": "1dccdc93-7ab6-4e28-e9a0-4d6d5878a8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20000, Loss: 0.03060833178460598\n"
          ]
        }
      ],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 4  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-10, 10, steps=100000).unsqueeze(1)\n",
        "\n",
        "y_train = gelu(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "\n",
        "# Plot loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\n",
        "plt.title('Loss Progression over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = gelu(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='True Gelu Function', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "plt.title('Comparison between True Swish Function and Model Prediction')\n",
        "plt.xlabel('Input x')\n",
        "plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAjkH75MMuKG"
      },
      "source": [
        "K=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1fXPYQYKNAwT",
        "outputId": "eae4c998-6328-44b9-b4a8-83245692a51f"
      },
      "outputs": [],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 8  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-10, 10, steps=100000).unsqueeze(1)\n",
        "\n",
        "y_train = gelu(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "\n",
        "# Plot loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\n",
        "plt.title('Loss Progression over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = gelu(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='True Gelu Function', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "plt.title('Comparison between True Swish Function and Model Prediction')\n",
        "plt.xlabel('Input x')\n",
        "plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJD9JfQFMvhx"
      },
      "source": [
        "K=12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pUgxfsjOP_7_",
        "outputId": "96cd2469-126f-4894-be43-4690cc1b3918"
      },
      "outputs": [],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 12  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-10, 10, steps=100000).unsqueeze(1)\n",
        "\n",
        "y_train = gelu(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "\n",
        "# Plot loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\n",
        "plt.title('Loss Progression over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = gelu(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='True Gelu Function', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "plt.title('Comparison between True Swish Function and Model Prediction')\n",
        "plt.xlabel('Input x')\n",
        "plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqJSk5SjMwZY"
      },
      "source": [
        "K=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXgCV4pdQBQn",
        "outputId": "eeccba08-916f-457f-fe1f-44f08f811c93"
      },
      "outputs": [],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 16  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-10, 10, steps=100000).unsqueeze(1)\n",
        "\n",
        "y_train = gelu(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "\n",
        "# Plot loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\n",
        "plt.title('Loss Progression over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = gelu(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='True Gelu Function', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "plt.title('Comparison between True Swish Function and Model Prediction')\n",
        "plt.xlabel('Input x')\n",
        "plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOLoAf2/n7uwbFTrhFQBpHD",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
