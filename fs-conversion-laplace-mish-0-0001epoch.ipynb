{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-17T02:48:33.148542Z","iopub.execute_input":"2024-05-17T02:48:33.148916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def relu(x):\n    return np.maximum(0, x)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef swish(x):\n    return x * 1 / (1 + np.exp(-x))\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n\n\ndef softplus(x, beta=1):\n    return (1 / beta) * np.log(1 + np.exp(beta * x))\n\n\ndef mish(x, beta=1):\n    return x * np.tanh(softplus(x, beta))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementation of spike function for PyTorch custom gradient\nclass SpikeFunction(autograd.Function):\n    @staticmethod\n    def forward(ctx, v_scaled):\n        z_ = torch.where(v_scaled > 0, torch.ones_like(v_scaled), torch.zeros_like(v_scaled))\n        ctx.save_for_backward(v_scaled)\n        return z_\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        v_scaled, = ctx.saved_tensors\n        dz_dv_scaled = torch.maximum(1 - torch.abs(v_scaled), torch.tensor(0.0, device=v_scaled.device))\n        dE_dv_scaled = grad_output * dz_dv_scaled\n        return dE_dv_scaled\n\n# Call spike function for PyTorch\ndef spike_function(v_scaled):\n    return SpikeFunction.apply(v_scaled)\n\n# FS class definition\nclass FS(nn.Module):\n    def __init__(self, num_params):\n        super(FS, self).__init__()\n        self.num_params = num_params\n        self.init_params()\n\n    def init_params(self):\n        # Sigmoid\n        if num_params == 4:\n            h_base = torch.tensor([6.1677, 3.0448, 1.5416, 0.5933])\n            d_base = torch.tensor([6.2078, 3.0905, 1.5505, 0.7675])\n            T_base = torch.tensor([5.8081, 2.7347, 1.2315, 0.4506])\n        elif num_params == 8:\n            h_base = torch.tensor([5.5101, 5.2136, 2.1780, 1.4478, 0.5394, 0.3573, 0.2208, 0.8026])\n            d_base = torch.tensor([5.5253, 5.3059, 2.2599, 1.4765, 0.5437, 0.3762, 0.2454, 0.1443])\n            T_base = torch.tensor([7.4063, 5.1803, 2.1767, 1.4740, 0.5572, 0.4120, 0.3017, 0.2140])\n        elif num_params == 12:\n            h_base = torch.tensor([2.1797, 3.7731, 1.8194, 1.9065, 1.1129, 0.6863, 0.7400, 0.4240, 0.4932, 0.2876, 0.1765, 0.8837])\n            d_base = torch.tensor([2.2590, 3.7766, 1.8347, 1.9014, 1.1138, -0.2289, 0.7274, 0.4129, 0.3883, 0.2060, 0.1063, 0.0580])\n            T_base = torch.tensor([2.6306, 3.9119, 2.0056, 2.3199, 1.4134, -3.4059, 0.2356, -0.1442, -0.4215, -0.7736, -0.8606, -0.9668])\n        elif num_params == 16:\n            h_base = torch.tensor([0.4598, 1.6207, 3.5916, 5.0855, 3.5037, 1.4011, 0.5193, 0.3566, 0.3758, 0.2910, 0.2418, 0.1859, 0.1275, 0.1083, 0.0557, 0.2794])\n            d_base = torch.tensor([0.2992, 1.7027, 3.5909, 5.1120, 3.5285, 1.4279, 0.5192, -0.2300, 0.3801, 0.2931, 0.2388, 0.1863, 0.1397, 0.1011, 0.0696, 0.0445])\n            T_base = torch.tensor([0.0837, 1.6734, 5.8320, 4.7407, 3.1445, 1.1091, 0.2026, -3.3994, -0.2830, -0.3697, -0.4241, -0.4746, -0.5184, -0.5490, -0.5797, -0.5998])\n\n        # Use Laplace noise\n        self.h = nn.Parameter(h_base + torch.distributions.Laplace(0, 0.0001).sample(h_base.size()))\n        self.d = nn.Parameter(d_base + torch.distributions.Laplace(0, 0.0001).sample(d_base.size()))\n        self.T = nn.Parameter(T_base + torch.distributions.Laplace(0, 0.0001).sample(T_base.size()))\n    \n    def add_noise(self):\n        self.h.data += torch.distributions.Laplace(0, 0.0001).sample(self.h.size())\n        self.d.data += torch.distributions.Laplace(0, 0.0001).sample(self.d.size())\n        self.T.data += torch.distributions.Laplace(0, 0.0001).sample(self.T.size())\n\n    def forward(self, x):\n        v = x.clone()\n\n        # Initialize temporary output for FS spike neural network\n        temp_out = torch.zeros_like(v)\n\n        # Implement FS spike neural network\n        for t in range(len(self.T)):\n            #v_scaled = (v - self.T[t]) / (torch.abs(v) + 1)\n            v_scaled = v - self.T[t]\n            z = spike_function(v_scaled)\n            temp_out += z * self.d[t]\n            v = v - z * self.h[t]\n\n        return temp_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate model and initial setup\nnum_params = 4  # Select arbitrary number of parameters\nmodel = FS(num_params)\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Training data\nx_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n\ny_train = mish(x_train)  # Target value\n\ntorch.autograd.set_detect_anomaly(True)\n\n# Training loop\nloss_values = []\n\nepochs = 20000\nfor epoch in range(epochs):\n    model.add_noise()\n    \n    optimizer.zero_grad()  # Reset gradients\n    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n    loss.backward()  # Backpropagation\n    optimizer.step()  # Update weights\n\n    # Print loss every 1000 epochs\n    if epoch % 1000 == 0:\n       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n\n    if epoch % 200 == 0:\n       loss_values.append(loss.item())\n\n# Print final loss\nprint(\"Loss values:\", loss_values)\nprint(f'Final Loss: {loss.item()}')\nprint(f'Final h: {model.h.data}')\nprint(f'Final d: {model.d.data}')\nprint(f'Final T: {model.T.data}')\n\n# Plot loss values\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\nplt.title('Loss Progression over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Calculate model predictions\nwith torch.no_grad():  # No gradient computation needed\n    y_pred = model(x_train).squeeze()\n\n# True function values\ny_true = mish(x_train).squeeze()\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x_train.numpy(), y_true.numpy(), label='Mish', color='r')\nplt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n# plt.xlabel('Input x')\n# plt.ylabel('Output y')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate model and initial setup\nnum_params = 8  # Select arbitrary number of parameters\nmodel = FS(num_params)\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Training data\nx_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n\ny_train = mish(x_train)  # Target value\n\ntorch.autograd.set_detect_anomaly(True)\n\n# Training loop\nloss_values = []\n\nepochs = 20000\nfor epoch in range(epochs):\n    model.add_noise()\n    \n    optimizer.zero_grad()  # Reset gradients\n    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n    loss.backward()  # Backpropagation\n    optimizer.step()  # Update weights\n\n    # Print loss every 1000 epochs\n    if epoch % 1000 == 0:\n       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n\n    if epoch % 200 == 0:\n       loss_values.append(loss.item())\n\n# Print final loss\nprint(\"Loss values:\", loss_values)\nprint(f'Final Loss: {loss.item()}')\nprint(f'Final h: {model.h.data}')\nprint(f'Final d: {model.d.data}')\nprint(f'Final T: {model.T.data}')\n\n# Plot loss values\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\nplt.title('Loss Progression over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Calculate model predictions\nwith torch.no_grad():  # No gradient computation needed\n    y_pred = model(x_train).squeeze()\n\n# True function values\ny_true = mish(x_train).squeeze()\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x_train.numpy(), y_true.numpy(), label='Mish', color='r')\nplt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n# plt.xlabel('Input x')\n# plt.ylabel('Output y')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate model and initial setup\nnum_params = 12  # Select arbitrary number of parameters\nmodel = FS(num_params)\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Training data\nx_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n\ny_train = mish(x_train)  # Target value\n\ntorch.autograd.set_detect_anomaly(True)\n\n# Training loop\nloss_values = []\n\nepochs = 20000\nfor epoch in range(epochs):\n    model.add_noise()\n    \n    optimizer.zero_grad()  # Reset gradients\n    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n    loss.backward()  # Backpropagation\n    optimizer.step()  # Update weights\n\n    # Print loss every 1000 epochs\n    if epoch % 1000 == 0:\n       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n\n    if epoch % 200 == 0:\n       loss_values.append(loss.item())\n\n# Print final loss\nprint(\"Loss values:\", loss_values)\nprint(f'Final Loss: {loss.item()}')\nprint(f'Final h: {model.h.data}')\nprint(f'Final d: {model.d.data}')\nprint(f'Final T: {model.T.data}')\n\n# Plot loss values\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\nplt.title('Loss Progression over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Calculate model predictions\nwith torch.no_grad():  # No gradient computation needed\n    y_pred = model(x_train).squeeze()\n\n# True function values\ny_true = mish(x_train).squeeze()\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x_train.numpy(), y_true.numpy(), label='Mish', color='r')\nplt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n# plt.xlabel('Input x')\n# plt.ylabel('Output y')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate model and initial setup\nnum_params = 16  # Select arbitrary number of parameters\nmodel = FS(num_params)\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Training data\nx_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n\ny_train = mish(x_train)  # Target value\n\ntorch.autograd.set_detect_anomaly(True)\n\n# Training loop\nloss_values = []\n\nepochs = 20000\nfor epoch in range(epochs):\n    model.add_noise()\n    \n    optimizer.zero_grad()  # Reset gradients\n    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n    loss.backward()  # Backpropagation\n    optimizer.step()  # Update weights\n\n    # Print loss every 1000 epochs\n    if epoch % 1000 == 0:\n       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n\n    if epoch % 200 == 0:\n       loss_values.append(loss.item())\n\n# Print final loss\nprint(\"Loss values:\", loss_values)\nprint(f'Final Loss: {loss.item()}')\nprint(f'Final h: {model.h.data}')\nprint(f'Final d: {model.d.data}')\nprint(f'Final T: {model.T.data}')\n\n# Plot loss values\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\nplt.title('Loss Progression over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Calculate model predictions\nwith torch.no_grad():  # No gradient computation needed\n    y_pred = model(x_train).squeeze()\n\n# True function values\ny_true = mish(x_train).squeeze()\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x_train.numpy(), y_true.numpy(), label='Mish', color='r')\nplt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n# plt.xlabel('Input x')\n# plt.ylabel('Output y')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}